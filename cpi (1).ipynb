{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79heLMHLO_pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1246692-f1cf-4aa6-c31c-1b10af90fd05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install os-sys"
      ],
      "metadata": {
        "id": "UcslRcuNQdKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "42d3a1d9-2d5d-49ce-d09d-92ce0edda534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting os-sys\n",
            "  Downloading os_sys-2.1.4-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting pygubu (from os-sys)\n",
            "  Downloading pygubu-0.35.6-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from os-sys) (2024.2)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.10/dist-packages (from os-sys) (0.5.1)\n",
            "Collecting progress (from os-sys)\n",
            "  Downloading progress-1.6.tar.gz (7.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from os-sys) (4.66.6)\n",
            "Collecting progressbar (from os-sys)\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from os-sys) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from os-sys) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from os-sys) (1.16.0)\n",
            "Collecting jupyter (from os-sys)\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from os-sys) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from os-sys) (4.12.3)\n",
            "Collecting Eel (from os-sys)\n",
            "  Downloading eel-0.17.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting extract-zip (from os-sys)\n",
            "  Downloading extract_zip-1.0.0-py3-none-any.whl.metadata (403 bytes)\n",
            "INFO: pip is looking at multiple versions of os-sys to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting os-sys\n",
            "  Downloading os_sys-2.1.3-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading os_sys-2.1.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading os_sys-2.1.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading os_sys-2.1.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading os_sys-2.0.9-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading os_sys-2.0.8-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading os_sys-2.0.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "INFO: pip is still looking at multiple versions of os-sys to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading os_sys-2.0.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "  Downloading os_sys-2.0.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "  Downloading os_sys-2.0.4-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting webview (from os-sys)\n",
            "  Downloading webview-0.1.5.tar.gz (18 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install paddlepaddle\n",
        "!pip install paddleocr"
      ],
      "metadata": {
        "id": "K_aacyeaPWea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae54bc4-dc16-4a93-fa30-f62b1712565f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting paddlepaddle\n",
            "  Downloading paddlepaddle-2.6.2-cp310-cp310-manylinux1_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (10.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (4.4.2)\n",
            "Collecting astor (from paddlepaddle)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting opt-einsum==3.3.0 (from paddlepaddle)\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (3.20.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->paddlepaddle) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->paddlepaddle) (1.2.2)\n",
            "Downloading paddlepaddle-2.6.2-cp310-cp310-manylinux1_x86_64.whl (126.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Installing collected packages: opt-einsum, astor, paddlepaddle\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "Successfully installed astor-0.8.1 opt-einsum-3.3.0 paddlepaddle-2.6.2\n",
            "Collecting paddleocr\n",
            "  Downloading paddleocr-2.9.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from paddleocr) (2.0.6)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from paddleocr) (0.24.0)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from paddleocr) (0.4.0)\n",
            "Collecting pyclipper (from paddleocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting lmdb (from paddleocr)\n",
            "  Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.66.6)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (1.26.4)\n",
            "Collecting rapidfuzz (from paddleocr)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.10.0.84)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.10.0.84)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from paddleocr) (3.0.11)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from paddleocr) (10.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from paddleocr) (6.0.2)\n",
            "Collecting python-docx (from paddleocr)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.12.3)\n",
            "Requirement already satisfied: fonttools>=4.24.0 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.54.1)\n",
            "Collecting fire>=0.3.0 (from paddleocr)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from paddleocr) (2.32.3)\n",
            "Collecting albumentations==1.4.10 (from paddleocr)\n",
            "  Downloading albumentations-1.4.10-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting albucore==0.0.13 (from paddleocr)\n",
            "  Downloading albucore-0.0.13-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.13->paddleocr) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.13->paddleocr) (4.12.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.13->paddleocr) (4.10.0.84)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.4.10->paddleocr) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.4.10->paddleocr) (1.5.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.4.10->paddleocr) (2.9.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.3.0->paddleocr) (2.5.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (3.4.2)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->paddleocr) (2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug->paddleocr) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug->paddleocr) (3.8.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->paddleocr) (5.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->paddleocr) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->paddleocr) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->paddleocr) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->paddleocr) (2024.8.30)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations==1.4.10->paddleocr) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations==1.4.10->paddleocr) (2.23.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->paddleocr) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->paddleocr) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (2.8.2)\n",
            "Downloading paddleocr-2.9.1-py3-none-any.whl (544 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.7/544.7 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.13-py3-none-any.whl (8.5 kB)\n",
            "Downloading albumentations-1.4.10-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.9/161.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (912 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.2/912.2 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=6ae11ad46eeed33fe46ce1f85d62824173f3e11ebf22e1ba30bd93932fdb7f52\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built fire\n",
            "Installing collected packages: pyclipper, lmdb, rapidfuzz, python-docx, fire, albucore, albumentations, paddleocr\n",
            "  Attempting uninstall: albucore\n",
            "    Found existing installation: albucore 0.0.19\n",
            "    Uninstalling albucore-0.0.19:\n",
            "      Successfully uninstalled albucore-0.0.19\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.4.20\n",
            "    Uninstalling albumentations-1.4.20:\n",
            "      Successfully uninstalled albumentations-1.4.20\n",
            "Successfully installed albucore-0.0.13 albumentations-1.4.10 fire-0.7.0 lmdb-1.5.1 paddleocr-2.9.1 pyclipper-1.3.0.post6 python-docx-1.1.2 rapidfuzz-3.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moviepy SpeechRecognition pydub"
      ],
      "metadata": {
        "id": "FA5v-3WXPcY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce08542-cb26-430b-ea98-628d2d54dfd2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.36.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (10.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (75.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.8.30)\n",
            "Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.11.0 pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q google-generativeai"
      ],
      "metadata": {
        "id": "FyHxc91LQTaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52_tJA4F7Zjn",
        "outputId": "30f1b854-bb15-4b49-e05b-11b569bbac96",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.6)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (3.0.2)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load your image (grayscale) and apply threshold\n",
        "gray_img = cv2.imread('/content/gdrive/MyDrive/TP lecture/clear_pic.png', cv2.IMREAD_GRAYSCALE)  # Load your image\n",
        "ret, thresh = cv2.threshold(gray_img, 180, 255, cv2.THRESH_BINARY_INV)\n",
        "#thresh = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
        "blurred_img = cv2.GaussianBlur(thresh, (1, 1), 0)\n",
        "\n",
        "\n",
        "\n",
        "# Save the thresholded image to your Google Drive\n",
        "cv2.imwrite('/content/gdrive/MyDrive/TP lecture/threshold.png', blurred_img)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "XeJPP0ovPioC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "df4c0554-090e-4b88-eec6-be08657a5e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport cv2\\nimport numpy as np\\n\\n# Load your image (grayscale) and apply threshold\\ngray_img = cv2.imread('/content/gdrive/MyDrive/TP lecture/clear_pic.png', cv2.IMREAD_GRAYSCALE)  # Load your image\\nret, thresh = cv2.threshold(gray_img, 180, 255, cv2.THRESH_BINARY_INV)\\n#thresh = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 2)\\nblurred_img = cv2.GaussianBlur(thresh, (1, 1), 0)\\n\\n\\n\\n# Save the thresholded image to your Google Drive\\ncv2.imwrite('/content/gdrive/MyDrive/TP lecture/threshold.png', blurred_img)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from paddleocr import PaddleOCR, draw_ocr\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the OCR model\n",
        "ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
        "\n",
        "# Load the preprocessed image\n",
        "img_path = '/content/gdrive/MyDrive/TP lecture/threshold.png'\n",
        "img = cv2.imread(img_path)\n",
        "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "kernel = np.ones((8, 8), np.uint8)\n",
        "dilated_img = cv2.dilate(gray_img, kernel, iterations=1)\n",
        "\n",
        "# Find contours\n",
        "contours, _ = cv2.findContours(dilated_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Sort contours by y-coordinate\n",
        "contours = sorted(contours, key=lambda c: cv2.boundingRect(c)[1])\n",
        "\n",
        "# Group contours by similar y-coordinate\n",
        "grouped_contours = []\n",
        "current_group = []\n",
        "prev_y = None\n",
        "y_threshold = 10  # Adjust this value to change the sensitivity of line grouping\n",
        "\n",
        "for cnt in contours:\n",
        "    x, y, w, h = cv2.boundingRect(cnt)\n",
        "\n",
        "    if w > 5 and h > 5:  # Minimum width and height to consider as text\n",
        "        if prev_y is None or abs(y - prev_y) <= y_threshold:\n",
        "            current_group.append(cnt)\n",
        "        else:\n",
        "            grouped_contours.append(current_group)\n",
        "            current_group = [cnt]\n",
        "        prev_y = y\n",
        "\n",
        "if current_group:\n",
        "    grouped_contours.append(current_group)\n",
        "\n",
        "# List to store all detected text\n",
        "all_detected_text = []\n",
        "\n",
        "# Process each group of contours\n",
        "for group in grouped_contours:\n",
        "    # Find the bounding rectangle for the entire group\n",
        "    x_min = min(cv2.boundingRect(cnt)[0] for cnt in group)\n",
        "    y_min = min(cv2.boundingRect(cnt)[1] for cnt in group)\n",
        "    x_max = max(cv2.boundingRect(cnt)[0] + cv2.boundingRect(cnt)[2] for cnt in group)\n",
        "    y_max = max(cv2.boundingRect(cnt)[1] + cv2.boundingRect(cnt)[3] for cnt in group)\n",
        "\n",
        "    # Extract the region of interest (ROI) for the entire group\n",
        "    roi = img[y_min:y_max, x_min:x_max]\n",
        "    sharpened = cv2.filter2D(roi, -1, np.array([[0, -1, 0], [-1, 10, -1], [0, -1, 0]]))\n",
        "\n",
        "    # Apply PaddleOCR on the ROI\n",
        "    result = ocr.ocr(sharpened, cls=True)\n",
        "\n",
        "    # Collect the recognized text from the ROI\n",
        "    line_text = []\n",
        "    for idx in range(len(result)):\n",
        "        res = result[idx]\n",
        "        if res is not None:\n",
        "            for line in res:\n",
        "                line_text.append(line[1][0])\n",
        "\n",
        "    # Join all text in this line and add to the overall list\n",
        "    if line_text:\n",
        "        all_detected_text.append(' '.join(line_text)+'\\n')\n",
        "\n",
        "\n",
        "    # Draw rectangle around the group\n",
        "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "\n",
        "# Print all detected text\n",
        "print(\"Detected Text:\")\n",
        "for line in all_detected_text:\n",
        "    print(line)\n",
        "\n",
        "# Save the image with bounding boxes\n",
        "cv2.imwrite('/content/gdrive/MyDrive/TP lecture/segmented_text_grouped.png', img)\n",
        "\n",
        "# Display the final image with detected ROIs\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "84hOkqbMPmhJ",
        "outputId": "19e925ed-3466-48af-f5ae-f135dab363e2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom paddleocr import PaddleOCR, draw_ocr\\nimport cv2\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\n# Initialize the OCR model\\nocr = PaddleOCR(use_angle_cls=True, lang=\\'en\\')\\n\\n# Load the preprocessed image\\nimg_path = \\'/content/gdrive/MyDrive/TP lecture/threshold.png\\'\\nimg = cv2.imread(img_path)\\ngray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n\\nkernel = np.ones((8, 8), np.uint8)\\ndilated_img = cv2.dilate(gray_img, kernel, iterations=1)\\n\\n# Find contours\\ncontours, _ = cv2.findContours(dilated_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\\n\\n# Sort contours by y-coordinate\\ncontours = sorted(contours, key=lambda c: cv2.boundingRect(c)[1])\\n\\n# Group contours by similar y-coordinate\\ngrouped_contours = []\\ncurrent_group = []\\nprev_y = None\\ny_threshold = 10  # Adjust this value to change the sensitivity of line grouping\\n\\nfor cnt in contours:\\n    x, y, w, h = cv2.boundingRect(cnt)\\n\\n    if w > 5 and h > 5:  # Minimum width and height to consider as text\\n        if prev_y is None or abs(y - prev_y) <= y_threshold:\\n            current_group.append(cnt)\\n        else:\\n            grouped_contours.append(current_group)\\n            current_group = [cnt]\\n        prev_y = y\\n\\nif current_group:\\n    grouped_contours.append(current_group)\\n\\n# List to store all detected text\\nall_detected_text = []\\n\\n# Process each group of contours\\nfor group in grouped_contours:\\n    # Find the bounding rectangle for the entire group\\n    x_min = min(cv2.boundingRect(cnt)[0] for cnt in group)\\n    y_min = min(cv2.boundingRect(cnt)[1] for cnt in group)\\n    x_max = max(cv2.boundingRect(cnt)[0] + cv2.boundingRect(cnt)[2] for cnt in group)\\n    y_max = max(cv2.boundingRect(cnt)[1] + cv2.boundingRect(cnt)[3] for cnt in group)\\n\\n    # Extract the region of interest (ROI) for the entire group\\n    roi = img[y_min:y_max, x_min:x_max]\\n    sharpened = cv2.filter2D(roi, -1, np.array([[0, -1, 0], [-1, 10, -1], [0, -1, 0]]))\\n\\n    # Apply PaddleOCR on the ROI\\n    result = ocr.ocr(sharpened, cls=True)\\n\\n    # Collect the recognized text from the ROI\\n    line_text = []\\n    for idx in range(len(result)):\\n        res = result[idx]\\n        if res is not None:\\n            for line in res:\\n                line_text.append(line[1][0])\\n\\n    # Join all text in this line and add to the overall list\\n    if line_text:\\n        all_detected_text.append(\\' \\'.join(line_text)+\\'\\n\\')\\n\\n\\n    # Draw rectangle around the group\\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\\n\\n# Print all detected text\\nprint(\"Detected Text:\")\\nfor line in all_detected_text:\\n    print(line)\\n\\n# Save the image with bounding boxes\\ncv2.imwrite(\\'/content/gdrive/MyDrive/TP lecture/segmented_text_grouped.png\\', img)\\n\\n# Display the final image with detected ROIs\\nplt.figure(figsize=(10, 10))\\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\\nplt.axis(\\'off\\')\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIDEO IMPLEMENTATION OF PADDLEOCR"
      ],
      "metadata": {
        "id": "Q9htPDl6QB_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from paddleocr import PaddleOCR\n",
        "import logging\n",
        "\n",
        "# Suppress PaddleOCR debug output\n",
        "logging.getLogger(\"ppocr\").setLevel(logging.WARNING)\n",
        "\n",
        "def process_video_to_text(video_path, output_folder):\n",
        "    def preprocess_image(image):\n",
        "        gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        ret, thresh = cv2.threshold(gray_img, 180, 255, cv2.THRESH_BINARY_INV)\n",
        "        blurred_img = cv2.GaussianBlur(thresh, (1, 1), 0)\n",
        "        return blurred_img\n",
        "\n",
        "    def extract_text_from_frame(frame, ocr):\n",
        "        preprocessed = preprocess_image(frame)\n",
        "        gray_img = cv2.cvtColor(preprocessed, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "        kernel = np.ones((8, 8), np.uint8)\n",
        "        dilated_img = cv2.dilate(cv2.cvtColor(gray_img, cv2.COLOR_BGR2GRAY), kernel, iterations=1)\n",
        "\n",
        "        contours, _ = cv2.findContours(dilated_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        #contours are sorted by their y-coordinate\n",
        "        contours = sorted(contours, key=lambda c: cv2.boundingRect(c)[1])\n",
        "\n",
        "        grouped_contours = []\n",
        "        current_group = []\n",
        "        prev_y = None\n",
        "        y_threshold = 10\n",
        "\n",
        "        for cnt in contours:\n",
        "            x, y, w, h = cv2.boundingRect(cnt)\n",
        "            if w > 5 and h > 5:\n",
        "                if prev_y is None or abs(y - prev_y) <= y_threshold:\n",
        "                    current_group.append(cnt)\n",
        "                else:\n",
        "                    grouped_contours.append(current_group)\n",
        "                    current_group = [cnt]\n",
        "                prev_y = y\n",
        "\n",
        "        if current_group:\n",
        "            grouped_contours.append(current_group)\n",
        "\n",
        "        all_detected_text = []\n",
        "\n",
        "        for group in grouped_contours:\n",
        "            x_min = min(cv2.boundingRect(cnt)[0] for cnt in group)\n",
        "            y_min = min(cv2.boundingRect(cnt)[1] for cnt in group)\n",
        "            x_max = max(cv2.boundingRect(cnt)[0] + cv2.boundingRect(cnt)[2] for cnt in group)\n",
        "            y_max = max(cv2.boundingRect(cnt)[1] + cv2.boundingRect(cnt)[3] for cnt in group)\n",
        "\n",
        "            roi = gray_img[y_min:y_max, x_min:x_max]\n",
        "            sharpened = cv2.filter2D(roi, -1, np.array([[0, -1, 0], [-1, 10, -1], [0, -1, 0]]))\n",
        "\n",
        "            result = ocr.ocr(sharpened, cls=True)\n",
        "\n",
        "            line_text = []\n",
        "            for idx in range(len(result)):\n",
        "                res = result[idx]\n",
        "                if res is not None:\n",
        "                    for line in res:\n",
        "                        line_text.append(line[1][0])\n",
        "\n",
        "            if line_text:\n",
        "                all_detected_text.append(' '.join(line_text))\n",
        "\n",
        "        return all_detected_text\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_interval = fps * 15\n",
        "\n",
        "    frame_count = 0\n",
        "    all_text = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % frame_interval == 0:\n",
        "            detected_text = extract_text_from_frame(frame, ocr)\n",
        "            all_text.extend(detected_text)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    consolidated_text_path = os.path.join(output_folder, 'consolidated_text.txt')\n",
        "    with open(consolidated_text_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(all_text))\n",
        "\n",
        "    return consolidated_text_path\n",
        "\n",
        "# Example usage\n",
        "#video_path = '/content/gdrive/MyDrive/TP lecture/TP_lecture.mp4'\n",
        "#output_folder = '/content/gdrive/MyDrive/TP lecture/many_imgs/'\n",
        "#consolidated_text_path = process_video_to_text(video_path, output_folder)\n",
        "\n",
        "#print(f\"Processing complete. Consolidated text saved to {consolidated_text_path}.\")\n"
      ],
      "metadata": {
        "id": "_V7EZHVIQErw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUDIO TRANSCRIPTION"
      ],
      "metadata": {
        "id": "SUnwH_5GQJmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import moviepy.editor as mp\n",
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "\n",
        "def video_to_speech_text(video_path, output_folder):\n",
        "    def video_to_audio(video_path, audio_path):\n",
        "        video = mp.VideoFileClip(video_path)\n",
        "        video.audio.write_audiofile(audio_path)\n",
        "\n",
        "    def transcribe_audio(audio_path, output_folder):\n",
        "        r = sr.Recognizer()\n",
        "\n",
        "        # Load audio file\n",
        "        sound = AudioSegment.from_wav(audio_path)\n",
        "\n",
        "        # Split audio where the silence is 700 milliseconds or more and get chunks\n",
        "        chunks = split_on_silence(sound,\n",
        "            min_silence_len = 700,\n",
        "            silence_thresh = sound.dBFS-14,\n",
        "            keep_silence=700,\n",
        "        )\n",
        "\n",
        "        # Process each chunk\n",
        "        whole_text = \"\"\n",
        "        for i, audio_chunk in enumerate(chunks, start=1):\n",
        "            # Export audio chunk and save it in a temporary file\n",
        "            chunk_filename = os.path.join(output_folder, f\"chunk{i}.wav\")\n",
        "            audio_chunk.export(chunk_filename, format=\"wav\")\n",
        "\n",
        "            # Recognize the chunk\n",
        "            with sr.AudioFile(chunk_filename) as source:\n",
        "                audio_listened = r.record(source)\n",
        "                try:\n",
        "                    text = r.recognize_google(audio_listened)\n",
        "                except sr.UnknownValueError as e:\n",
        "                    print(\"Error:\", str(e))\n",
        "                else:\n",
        "                    text = f\"{text.capitalize()}. \"\n",
        "                    whole_text += text\n",
        "\n",
        "            # Remove temporary file\n",
        "            os.remove(chunk_filename)\n",
        "\n",
        "        # Return the text for all chunks detected\n",
        "        return whole_text\n",
        "\n",
        "    audio_path = os.path.join(output_folder, 'extracted_audio.wav')\n",
        "    text_output_path = os.path.join(output_folder, 'speech_text.txt')\n",
        "\n",
        "    # Create output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Step 1: Extract audio from video\n",
        "    #print(\"Extracting audio from video...\")\n",
        "    video_to_audio(video_path, audio_path)\n",
        "\n",
        "    # Step 2: Transcribe audio to text\n",
        "    #print(\"Transcribing audio to text... This may take a while.\")\n",
        "    transcribed_text = transcribe_audio(audio_path, output_folder)\n",
        "\n",
        "    # Step 3: Save transcribed text to file\n",
        "    with open(text_output_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(transcribed_text)\n",
        "\n",
        "    #print(f\"Speech-to-text conversion complete. Result saved to {text_output_path}\")\n",
        "\n",
        "    # Optional: Remove the temporary audio file\n",
        "    os.remove(audio_path)\n",
        "\n",
        "    return text_output_path\n",
        "\n",
        "# Example usage\n",
        "\"\"\"\n",
        "video_path = '/content/gdrive/MyDrive/TP lecture/TP_lecture.mp4'\n",
        "output_folder = '/content/gdrive/MyDrive/TP lecture/many_imgs'\n",
        "speech_text_path = video_to_speech_text(video_path, output_folder)\n",
        "\n",
        "print(f\"Processing complete. Speech-to-text result saved to {speech_text_path}.\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0alLjjEmQLKI",
        "outputId": "8d6c7c26-6e77-4ac9-fc39-d2c9756ce952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nvideo_path = \\'/content/gdrive/MyDrive/TP lecture/TP_lecture.mp4\\'\\noutput_folder = \\'/content/gdrive/MyDrive/TP lecture/many_imgs\\'\\nspeech_text_path = video_to_speech_text(video_path, output_folder)\\n\\nprint(f\"Processing complete. Speech-to-text result saved to {speech_text_path}.\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GEMINI IMPLEMENTATION"
      ],
      "metadata": {
        "id": "IWqYjpg3QPmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the client library and import necessary modules.\n",
        "import google.generativeai as genai\n",
        "\n",
        "import base64\n",
        "import copy\n",
        "import hashlib\n",
        "import io\n",
        "import json\n",
        "import mimetypes\n",
        "import pathlib\n",
        "import pprint\n",
        "import requests\n",
        "\n",
        "\n",
        "import PIL.Image\n",
        "import IPython.display\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "OjKu2dE-QWSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace(\".\", \" *\")\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "GkCTd54XQZBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install markdown2 weasyprint\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Cb26SeRCUYr-",
        "outputId": "e881ba7a-48af-4297-a3f4-32b533321051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting markdown2\n",
            "  Downloading markdown2-2.5.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting weasyprint\n",
            "  Downloading weasyprint-63.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting pydyf<0.12,>=0.11.0 (from weasyprint)\n",
            "  Downloading pydyf-0.11.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: cffi<2,>=0.6 in /usr/local/lib/python3.10/dist-packages (from weasyprint) (1.17.1)\n",
            "Collecting tinyhtml5<3,>=2.0.0b1 (from weasyprint)\n",
            "  Downloading tinyhtml5-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: tinycss2<2,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from weasyprint) (1.4.0)\n",
            "Collecting cssselect2<0.8,>=0.1 (from weasyprint)\n",
            "  Downloading cssselect2-0.7.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting Pyphen<0.16,>=0.9.1 (from weasyprint)\n",
            "  Downloading pyphen-0.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: Pillow<11,>=9.1.0 in /usr/local/lib/python3.10/dist-packages (from weasyprint) (10.4.0)\n",
            "Requirement already satisfied: fonttools<5,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from fonttools[woff]<5,>=4.0.0->weasyprint) (4.54.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2,>=0.6->weasyprint) (2.22)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from cssselect2<0.8,>=0.1->weasyprint) (0.5.1)\n",
            "Collecting zopfli>=0.1.4 (from fonttools[woff]<5,>=4.0.0->weasyprint)\n",
            "  Downloading zopfli-0.2.3.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting brotli>=1.0.1 (from fonttools[woff]<5,>=4.0.0->weasyprint)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
            "Downloading markdown2-2.5.1-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading weasyprint-63.0-py3-none-any.whl (299 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.9/299.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect2-0.7.0-py3-none-any.whl (15 kB)\n",
            "Downloading pydyf-0.11.0-py3-none-any.whl (8.1 kB)\n",
            "Downloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tinyhtml5-2.0.0-py3-none-any.whl (39 kB)\n",
            "Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zopfli-0.2.3.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.1/849.1 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, zopfli, tinyhtml5, Pyphen, pydyf, markdown2, cssselect2, weasyprint\n",
            "Successfully installed Pyphen-0.15.0 brotli-1.1.0 cssselect2-0.7.0 markdown2-2.5.1 pydyf-0.11.0 tinyhtml5-2.0.0 weasyprint-63.0 zopfli-0.2.3.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "set up api"
      ],
      "metadata": {
        "id": "N-AOQ4kKQb57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import markdown2\n",
        "from weasyprint import HTML, CSS\n",
        "import google.generativeai as genai\n",
        "import logging\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.WARNING)\n",
        "\n",
        "def create_lecture_notes_pdf(ocr_text_file, speech_text_file, output_folder):\n",
        "    os.environ['GOOGLE_API_KEY'] = \"ADD YOUR GEMINI API KEY HERE\"\n",
        "    genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "    with open(ocr_text_file, 'r') as file1, open(speech_text_file, 'r') as file2:\n",
        "        ocr_text = file1.read()\n",
        "        speech_text = file2.read()\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    I'm creating notes for a lecture. I have added the OCR extracted text from the presentation and the speech transcript by the professor.\n",
        "    File 1 contains the ocr_text, a lot of it is repeated and not clear, please use contextual understanding to figure it out. File 2 is the speech\n",
        "    transcript. Create textbook like notes for the lecture, and keep keywords, important metaphors, names etc the same.\n",
        "    File 1:\n",
        "    {ocr_text}\n",
        "\n",
        "    File 2:\n",
        "    {speech_text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    css = \"\"\"\n",
        "    @page { size: A4; margin: 1cm }\n",
        "    body { font-family: 'Arial', sans-serif; font-size: 14px; }\n",
        "    h1 { font-family: 'Times New Roman', serif; font-size: 24px; }\n",
        "    h2 { font-family: 'Times New Roman', serif; font-size: 20px; }\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert markdown to HTML\n",
        "    html_text = markdown2.markdown(response.text)\n",
        "\n",
        "    # Save the HTML to a file\n",
        "    html_output_path = os.path.join(output_folder, \"output.html\")\n",
        "    with open(html_output_path, \"w\") as f:\n",
        "        f.write(html_text)\n",
        "\n",
        "    # Convert HTML to PDF\n",
        "    pdf_output_path = os.path.join(output_folder, \"notes.pdf\")\n",
        "    HTML(html_output_path).write_pdf(pdf_output_path, stylesheets=[CSS(string=css)])\n",
        "\n",
        "    print(f\"Markdown has been converted to PDF and saved as {pdf_output_path}\")\n",
        "\n",
        "    return pdf_output_path\n",
        "\n",
        "# Example usage\n",
        "\n",
        "video_path = '/content/gdrive/MyDrive/TP lecture/TP_lecture.mp4'\n",
        "output_folder = '/content/gdrive/MyDrive/TP lecture/many_imgs'\n",
        "\n",
        "#pdf_path = create_lecture_notes_pdf(process_video_to_text(video_path, output_folder), video_to_speech_text(video_path, output_folder), output_folder)\n",
        "\n",
        "#ocr_text_file = '/content/gdrive/MyDrive/TP lecture/many_imgs/consolidated_text.txt'\n",
        "#speech_text_file = '/content/gdrive/MyDrive/TP lecture/many_imgs/speech_text.txt'\n",
        "#output_folder = '/content/gdrive/MyDrive/TP lecture/many_imgs'\n",
        "\n",
        "#pdf_path = create_lecture_notes_pdf(ocr_text_file, speech_text_file, output_folder)\n",
        "#print(f\"Lecture notes PDF saved at {pdf_path}\")\n"
      ],
      "metadata": {
        "id": "tjWqi3NsQiSw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bo0jnWob9oS",
        "outputId": "0ddc2379-8067-4145-b4c2-6c33f514c088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-cors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E70EA4pgBLo1",
        "outputId": "c7b5de78-ad7e-49af-d173-774c0edca380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.10/dist-packages (5.0.0)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from flask-cors) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.0.6)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.9->flask-cors) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken ADD YOUR NGROK AUTHETNTICAN TOKEN HERE\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IQJnvSkY3ue",
        "outputId": "cd93cfb6-8789-42b8-c228-dccc09b9cecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, send_file\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route('/upload', methods=['POST'])\n",
        "def upload_file():\n",
        "    try:\n",
        "        if 'file' not in request.files:\n",
        "            return 'No file part', 400\n",
        "\n",
        "        file = request.files['file']\n",
        "        if file.filename == '':\n",
        "            return 'No selected file', 400\n",
        "\n",
        "        if file and file.filename.endswith('.mp4'):\n",
        "            # Ensure the directory exists\n",
        "            os.makedirs('/content', exist_ok=True)\n",
        "\n",
        "            video_path = os.path.join('/content', file.filename)\n",
        "            file.save(video_path)\n",
        "            print(f\"Received! -> {file.filename}\")  # This will print in Colab\n",
        "\n",
        "            # Path to the PDF file to be sent back\n",
        "            #pdf_path = '/content/gdrive/MyDrive/FeeReciept2023.pdf'\n",
        "            output_folder = '/content/gdrive/MyDrive/TP lecture/many_imgs'\n",
        "\n",
        "            print(\"Processing the video to text......\")\n",
        "            lecture_text = process_video_to_text(video_path, output_folder)\n",
        "            print(\"Processing the speech to text......\")\n",
        "            video_text = video_to_speech_text(video_path, output_folder)\n",
        "            pdf_path = create_lecture_notes_pdf(lecture_text, video_text, output_folder)\n",
        "\n",
        "\n",
        "\n",
        "            # Send the PDF file back to the client\n",
        "            return send_file(\n",
        "                pdf_path,\n",
        "                mimetype='application/pdf',\n",
        "                as_attachment=True,\n",
        "                download_name='FeeReciept2023.pdf'\n",
        "            )\n",
        "        else:\n",
        "            return 'Invalid file type. Please upload an MP4 file.', 400\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return f'Error processing file: {str(e)}', 500\n",
        "\n",
        "# Setup ngrok\n",
        "\n",
        "public_url = ngrok.connect(5000)\n",
        "print('Public URL:', public_url)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "LIRu4iMZ7U1H",
        "outputId": "1596e840-a14e-4f00-a023-5479c4b637ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://dcfa-34-169-114-186.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received! -> short clip.mp4\n",
            "Processing the video to text......\n",
            "Processing the speech to text......\n",
            "MoviePy - Writing audio in /content/gdrive/MyDrive/TP lecture/many_imgs/extracted_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Error: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:fontTools.ttLib.ttFont:Reading 'maxp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'maxp' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.004s to load 'maxp'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'maxp'\n",
            "INFO:fontTools.subset:maxp pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'cmap' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'cmap' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'post' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'post' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.010s to load 'cmap'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'cmap'\n",
            "INFO:fontTools.subset:cmap pruned\n",
            "INFO:fontTools.subset:fpgm dropped\n",
            "INFO:fontTools.subset:prep dropped\n",
            "INFO:fontTools.subset:cvt  dropped\n",
            "INFO:fontTools.subset:kern dropped\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to load 'post'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'post'\n",
            "INFO:fontTools.subset:post pruned\n",
            "INFO:fontTools.subset:GPOS dropped\n",
            "INFO:fontTools.subset:GSUB dropped\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'glyf' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'glyf' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'loca' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'loca' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'head' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'head' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.008s to load 'glyf'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to close glyph list over 'cmap'\n",
            "INFO:fontTools.subset:Added gid0 to subset\n",
            "INFO:fontTools.subset:Closing glyph list over 'glyf': 22 glyphs before\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'P', 'S', 'T', 'a', 'c', 'colon', 'd', 'e', 'f', 'g', 'h', 'i', 'm', 'n', 'o', 'p', 'r', 's', 't', 'uni00A0']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 29, 36, 51, 54, 55, 68, 70, 71, 72, 73, 74, 75, 76, 80, 81, 82, 83, 85, 86, 87]\n",
            "INFO:fontTools.subset:Closed glyph list over 'glyf': 22 glyphs after\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'P', 'S', 'T', 'a', 'c', 'colon', 'd', 'e', 'f', 'g', 'h', 'i', 'm', 'n', 'o', 'p', 'r', 's', 't', 'uni00A0']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 29, 36, 51, 54, 55, 68, 70, 71, 72, 73, 74, 75, 76, 80, 81, 82, 83, 85, 86, 87]\n",
            "DEBUG:fontTools.subset.timer:Took 0.025s to close glyph list over 'glyf'\n",
            "INFO:fontTools.subset:Retaining 22 glyphs\n",
            "INFO:fontTools.subset:head subsetting not needed\n",
            "INFO:fontTools.subset:hhea subsetting not needed\n",
            "INFO:fontTools.subset:maxp subsetting not needed\n",
            "INFO:fontTools.subset:OS/2 subsetting not needed\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hmtx' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hmtx' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hhea' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hhea' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.005s to subset 'hmtx'\n",
            "INFO:fontTools.subset:hmtx subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'cmap'\n",
            "INFO:fontTools.subset:cmap subsetted\n",
            "INFO:fontTools.subset:loca subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'post'\n",
            "INFO:fontTools.subset:post subsetted\n",
            "INFO:fontTools.subset:gasp subsetting not needed\n",
            "INFO:fontTools.subset:FFTM NOT subset; don't know how to subset\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'GDEF' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'GDEF' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.003s to subset 'GDEF'\n",
            "INFO:fontTools.subset:GDEF subsetted\n",
            "INFO:fontTools.subset:name subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.001s to subset 'glyf'\n",
            "INFO:fontTools.subset:glyf subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset GlyphOrder\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'head'\n",
            "INFO:fontTools.subset:head pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'OS/2' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'OS/2' table\n",
            "INFO:fontTools.subset:OS/2 Unicode ranges pruned: [0, 1]\n",
            "INFO:fontTools.subset:OS/2 CodePage ranges pruned: [0]\n",
            "DEBUG:fontTools.subset.timer:Took 0.001s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'GDEF'\n",
            "INFO:fontTools.subset:GDEF pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'name' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'name' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'gasp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'gasp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'FFTM' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'FFTM' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.007s to prune 'name'\n",
            "INFO:fontTools.subset:name pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'glyf' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'glyf' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'maxp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'maxp' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'loca' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'loca' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'head' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'head' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'hmtx' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'hmtx' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'hhea' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'hhea' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'OS/2' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'OS/2' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'cmap' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'cmap' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'name' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'name' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'post' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'post' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'gasp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'gasp' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'FFTM' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'FFTM' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'GDEF' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'GDEF' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'maxp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'maxp' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.009s to load 'maxp'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'maxp'\n",
            "INFO:fontTools.subset:maxp pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'cmap' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'cmap' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'post' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'post' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.006s to load 'cmap'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'cmap'\n",
            "INFO:fontTools.subset:cmap pruned\n",
            "INFO:fontTools.subset:fpgm dropped\n",
            "INFO:fontTools.subset:prep dropped\n",
            "INFO:fontTools.subset:cvt  dropped\n",
            "INFO:fontTools.subset:kern dropped\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to load 'post'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'post'\n",
            "INFO:fontTools.subset:post pruned\n",
            "INFO:fontTools.subset:GPOS dropped\n",
            "INFO:fontTools.subset:GSUB dropped\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'glyf' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'glyf' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'loca' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'loca' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'head' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'head' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.008s to load 'glyf'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to close glyph list over 'cmap'\n",
            "INFO:fontTools.subset:Added gid0 to subset\n",
            "INFO:fontTools.subset:Closing glyph list over 'glyf': 45 glyphs before\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'I', 'M', 'P', 'T', 'W', 'Y', 'a', 'b', 'bullet', 'c', 'colon', 'comma', 'd', 'e', 'f', 'four', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'period', 'q', 'quotedbl', 'quotesingle', 'r', 's', 't', 'u', 'uni00A0', 'uni00AD', 'uni037E', 'v', 'w', 'x', 'y', 'z', 'zero']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 5, 10, 15, 16, 17, 19, 23, 29, 30, 36, 44, 48, 51, 55, 58, 60, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 535]\n",
            "INFO:fontTools.subset:Closed glyph list over 'glyf': 45 glyphs after\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'I', 'M', 'P', 'T', 'W', 'Y', 'a', 'b', 'bullet', 'c', 'colon', 'comma', 'd', 'e', 'f', 'four', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'period', 'q', 'quotedbl', 'quotesingle', 'r', 's', 't', 'u', 'uni00A0', 'uni00AD', 'uni037E', 'v', 'w', 'x', 'y', 'z', 'zero']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 5, 10, 15, 16, 17, 19, 23, 29, 30, 36, 44, 48, 51, 55, 58, 60, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 535]\n",
            "DEBUG:fontTools.subset.timer:Took 0.008s to close glyph list over 'glyf'\n",
            "INFO:fontTools.subset:Retaining 45 glyphs\n",
            "INFO:fontTools.subset:head subsetting not needed\n",
            "INFO:fontTools.subset:hhea subsetting not needed\n",
            "INFO:fontTools.subset:maxp subsetting not needed\n",
            "INFO:fontTools.subset:OS/2 subsetting not needed\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hmtx' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hmtx' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hhea' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hhea' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.005s to subset 'hmtx'\n",
            "INFO:fontTools.subset:hmtx subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'cmap'\n",
            "INFO:fontTools.subset:cmap subsetted\n",
            "INFO:fontTools.subset:loca subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'post'\n",
            "INFO:fontTools.subset:post subsetted\n",
            "INFO:fontTools.subset:gasp subsetting not needed\n",
            "INFO:fontTools.subset:FFTM NOT subset; don't know how to subset\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'GDEF' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'GDEF' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.003s to subset 'GDEF'\n",
            "INFO:fontTools.subset:GDEF subsetted\n",
            "INFO:fontTools.subset:name subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.003s to subset 'glyf'\n",
            "INFO:fontTools.subset:glyf subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset GlyphOrder\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'head'\n",
            "INFO:fontTools.subset:head pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'OS/2' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'OS/2' table\n",
            "INFO:fontTools.subset:OS/2 Unicode ranges pruned: [0, 1, 7, 31]\n",
            "INFO:fontTools.subset:OS/2 CodePage ranges pruned: [0]\n",
            "DEBUG:fontTools.subset.timer:Took 0.001s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'GDEF'\n",
            "INFO:fontTools.subset:GDEF pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'name' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'name' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'gasp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'gasp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'FFTM' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'FFTM' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.007s to prune 'name'\n",
            "INFO:fontTools.subset:name pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'glyf' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'glyf' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'maxp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'maxp' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'loca' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'loca' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'head' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'head' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'hmtx' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'hmtx' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'hhea' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'hhea' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'OS/2' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'OS/2' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'cmap' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'cmap' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'name' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'name' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'post' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'post' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'gasp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'gasp' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'FFTM' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'FFTM' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'GDEF' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'GDEF' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'maxp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'maxp' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.007s to load 'maxp'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'maxp'\n",
            "INFO:fontTools.subset:maxp pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'cmap' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'cmap' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'post' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'post' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.009s to load 'cmap'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'cmap'\n",
            "INFO:fontTools.subset:cmap pruned\n",
            "INFO:fontTools.subset:fpgm dropped\n",
            "INFO:fontTools.subset:prep dropped\n",
            "INFO:fontTools.subset:cvt  dropped\n",
            "INFO:fontTools.subset:kern dropped\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to load 'post'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'post'\n",
            "INFO:fontTools.subset:post pruned\n",
            "INFO:fontTools.subset:GPOS dropped\n",
            "INFO:fontTools.subset:GSUB dropped\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'glyf' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'glyf' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'loca' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'loca' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'head' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'head' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.011s to load 'glyf'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to close glyph list over 'cmap'\n",
            "INFO:fontTools.subset:Added gid0 to subset\n",
            "INFO:fontTools.subset:Closing glyph list over 'glyf': 33 glyphs before\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'E', 'H', 'P', 'S', 'T', 'a', 'asterisk', 'b', 'c', 'colon', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'period', 'plus', 'r', 's', 't', 'u', 'uni00A0', 'w', 'y']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 13, 14, 17, 29, 36, 40, 43, 51, 54, 55, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 90, 92]\n",
            "INFO:fontTools.subset:Closed glyph list over 'glyf': 33 glyphs after\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'E', 'H', 'P', 'S', 'T', 'a', 'asterisk', 'b', 'c', 'colon', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'period', 'plus', 'r', 's', 't', 'u', 'uni00A0', 'w', 'y']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 13, 14, 17, 29, 36, 40, 43, 51, 54, 55, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 90, 92]\n",
            "DEBUG:fontTools.subset.timer:Took 0.009s to close glyph list over 'glyf'\n",
            "INFO:fontTools.subset:Retaining 33 glyphs\n",
            "INFO:fontTools.subset:head subsetting not needed\n",
            "INFO:fontTools.subset:hhea subsetting not needed\n",
            "INFO:fontTools.subset:maxp subsetting not needed\n",
            "INFO:fontTools.subset:OS/2 subsetting not needed\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hmtx' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hmtx' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hhea' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hhea' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.005s to subset 'hmtx'\n",
            "INFO:fontTools.subset:hmtx subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'cmap'\n",
            "INFO:fontTools.subset:cmap subsetted\n",
            "INFO:fontTools.subset:loca subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'post'\n",
            "INFO:fontTools.subset:post subsetted\n",
            "INFO:fontTools.subset:gasp subsetting not needed\n",
            "INFO:fontTools.subset:FFTM NOT subset; don't know how to subset\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'GDEF' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'GDEF' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.005s to subset 'GDEF'\n",
            "INFO:fontTools.subset:GDEF subsetted\n",
            "INFO:fontTools.subset:name subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.001s to subset 'glyf'\n",
            "INFO:fontTools.subset:glyf subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset GlyphOrder\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'head'\n",
            "INFO:fontTools.subset:head pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'OS/2' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'OS/2' table\n",
            "INFO:fontTools.subset:OS/2 Unicode ranges pruned: [0, 1]\n",
            "INFO:fontTools.subset:OS/2 CodePage ranges pruned: [0]\n",
            "DEBUG:fontTools.subset.timer:Took 0.001s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'GDEF'\n",
            "INFO:fontTools.subset:GDEF pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'name' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'name' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'gasp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'gasp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'FFTM' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'FFTM' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.009s to prune 'name'\n",
            "INFO:fontTools.subset:name pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'glyf' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'glyf' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'maxp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'maxp' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'loca' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'loca' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'head' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'head' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'hmtx' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'hmtx' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'hhea' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'hhea' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'OS/2' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'OS/2' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'cmap' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'cmap' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'name' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'name' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'post' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'post' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'gasp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'gasp' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'FFTM' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'FFTM' table to disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Compiling 'GDEF' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Writing 'GDEF' table to disk\n",
            "INFO:werkzeug:127.0.0.1 - - [06/Nov/2024 04:15:44] \"POST /upload HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Markdown has been converted to PDF and saved as /content/gdrive/MyDrive/TP lecture/many_imgs/notes.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJtoCTc-YS8W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}